---
title: "Lumenize RPC vs Cap'n Web"
description: "Performance difference is <1% on latency for typical networks, so the decision is more about what features and design philosophy are a better fit for your situation."
slug: lumenize-rpc-vs-capn-web
tags: [rpc]
draft: true
image: ./images/cover.png
---

import TOCInline from '@theme/TOCInline';

<TOCInline toc={toc} maxHeadingLevel={2} />

> Draft note: This post is a living document while we validate a few open questions (promise pipelining behavior, capability-based security patterns, MessagePort mode, and connection break handling). It remains hidden from production builds via `draft: true`.

## Summary

Bottom line: Lumenize RPC is highly competitive with Cap'n Web (Cloudflare's official solution). On real-world networks (100+ Mbps), the ~0.142ms local overhead becomes <1% of total latency.

The remaining gap stems from protocol features that buy you developer experience and debuggability: full StructuredClone support (Map, Set, Date, RegExp, typed arrays), Error.stack preservation, and circular references. Our `routeDORequest` helper adds no measurable overhead.

### References

- Cap'n Web blog post by Kenton Varda: https://blog.cloudflare.com/capnweb-javascript-rpc-library/
- Cap'n Web README (GitHub): https://github.com/cloudflare/capnweb/blob/main/README.md

## Strongest design values: minimal round trips vs local feeling

Both systems use “magic” (Proxies, promise pipelining, etc.) to make RPC feel local. Both systems attempt to minimize round trips. Where they differ is in the relative value they place on those things.

Cap'n Web seems to most value doing more with a single round trip — its `map()` operator is a perfect example.

On the other hand, Lumenize RPC most values making it feel like you are working locally. Rethrowing errors on the calling side complete with the original stack trace is a great example. Support for accessing `this.ctx` and `this.env` is another. [TODO: need to confirm that Cap'n Web doesn't allow this. If it does allow this, keep this note, but shift the conversation around extensive examples of doing this in the Lumenize docs and none in the Cap'n Web docs].

The Cap'n Web concept of a session with state maintained on both sides is a more subtle distinction here. To avoid the risk of dangling resources or the complexity of manually needing to clean that up (See , Lumenize RPC would simply make another round trip.

- Lumenize RPC emphasizes explicitness and convention-over-magic for routing and state. We avoid maintaining remote object graphs where possible to reduce dangling resource concerns.
- Cap'n Web models object capabilities richly. That power comes with stub lifecycle rules: explicit disposal (`[Symbol.dispose]`, `using`), ownership transfer, `onRpcBroken()` listeners, duplication via `stub.dup()`, and batch/session scoping. See “Resource Management and Disposal” in the README.
- Our aim: fewer ways to accidentally “forget” a remote reference. We prefer short-lived calls, explicit return-address patterns, and avoiding long-lived remote stubs when not necessary.

We’ll validate whether any Lumenize RPC patterns can leak remote resources, and document guardrails if so.

## Security and auth patterns (capability style)

Cap'n Web recommends in-band authentication that returns an authenticated API (object capability). This aligns with our approach and Cloudflare’s realities (e.g., browser WebSocket auth constraints).

- Pattern: authenticate() → returns Authed API; subsequent calls are naturally authorized on that object.
- Lumenize RPC: we support the same in-band pattern. We’ll add examples mirroring this flow and pair it with runtime validation via TypeBox Value.
- Rate limiting and CPU limits: consider per-operation guards to prevent pipelined abuse on expensive methods (Cap'n Web also recommends this).

## Interoperability and transports

- Cap'n Web interoperates with Cloudflare Workers built-in RPC and supports MessagePort. Lumenize RPC is WebSocket-first; MessagePort is on our research list for when it provides real benefits (iframes/workers).
- We’ll include guidance on when to choose MessagePort vs WebSocket and call out the Window vs MessageChannel advice noted in Cap'n Web docs.

## Promise pipelining and `.map()`

- Both systems support promise pipelining to minimize round trips; we’ll confirm Lumenize’s chaining behavior under various transports.
- Cap'n Web’s special `.map()` records a transformation and replays it server-side to avoid extra trips. If you control the server API, an equivalent “compose it server-side” method often achieves the same with less complexity; `.map()` shines more when you can’t extend the server API.

<details>
<summary><strong>Performance details (expand)</strong></summary>

### What we measured

- Environment: Node.js v22.14.0, Wrangler 4.38.0, fresh WebSocket per op (fair baseline)
- Workload: Mixed operations, 100 total ops

| Implementation | Time | Throughput |
|---------------|------|------------|
| Lumenize RPC | 0.171ms/op | 5858 ops/sec |
| Cap'n Web | 0.156ms/op | 6414 ops/sec |
| Gap | 0.015ms | Lumenize 1.09x slower |

Finding: Within 9% of Cloudflare's official solution under identical conditions.

### Protocol trade-offs (why the tiny gap exists)

- Full StructuredClone compatibility (complex types “just work”)
- Complete Error.stack preservation across RPC boundaries
- Circular reference support
- Better error messages and DX

These account for ~0.142ms of local overhead in our tests.

### Real-world networks: why the gap rarely matters

Payloads are small (~350 bytes per op), so transfer time is negligible. Network base latency (DNS/TCP/TLS) dominates 95–99% of total time.

| Network | Bandwidth | Base Latency | Lumenize | Cap'n Web | Gap | Gap % |
|--------|-----------|--------------|----------|-----------|-----|-------|
| Datacenter | 1 Gbps | 1ms | 2.25ms | 2.11ms | 0.14ms | 6.6% |
| Broadband | 100 Mbps | 10ms | 20.30ms | 20.16ms | 0.14ms | 0.7% |
| Mobile | 50 Mbps | 30ms | 60.36ms | 60.22ms | 0.14ms | 0.2% |
| Slow | 10 Mbps | 50ms | 100.80ms | 100.66ms | 0.14ms | 0.1% |

Implications:
- The 0.14ms local gap is ~<1% on typical networks
- Serialization is not a bottleneck (≈5% of processing time)
- Optimizing cbor-x or similar nets <0.5% improvement — not worth it
- Focus on connection pooling (saves 10–50ms), not micro-optimizations

### Optimization strategy

- Macro-optimization: connection pooling and session reuse
- Micro-optimization: skip (impact is sub-1%)

</details>

## When to choose which

Use Lumenize RPC when:
- You want best-in-class DX: StructuredClone, Error.stack, circular refs, strong errors
- You’re on typical networks (100+ Mbps): local overhead is negligible
- You want WebSocket-first RPC with ergonomics and safety helpers

Use Cap'n Web when:
- You need absolute minimum local latency in datacenter scenarios
- Your payloads are simple (primitives/POJOs) and debugging needs are modest
- You prefer Cloudflare’s out-of-the-box RPC model and MessagePort integrations

## Decision checklist

- Are you on typical networks (100+ Mbps) where network latency dominates? → Lumenize RPC
- Need structured clone types, Error.stack, and circular refs to “just work”? → Lumenize RPC
- Want MessagePort interop or Cloudflare’s Workers RPC compatibility out of the box? → Cap'n Web
- Chasing lowest possible local latency in a datacenter-only scenario? → Cap'n Web
- Prefer convention-based routing with zero-measured overhead (`routeDORequest`)? → Lumenize RPC

## Feature comparison

<!-- HTML table keeps formatting stable while we iterate; edit freely -->
<table>
	<thead>
		<tr>
			<th>Capability</th>
			<th>Lumenize RPC</th>
			<th>Cap'n Web</th>
			<th>Notes</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>StructuredClone types (Map/Set/Date/RegExp/TypedArrays)</td>
			<td>Yes</td>
			<td>Partial (POJOs preferred)</td>
			<td>Lumenize preserves complex types end-to-end</td>
		</tr>
		<tr>
			<td>Error.stack preservation</td>
			<td>Yes</td>
			<td>Limited</td>
			<td>Improves DX and debugging across boundaries</td>
		</tr>
		<tr>
			<td>Circular reference support</td>
			<td>Yes</td>
			<td>Never</td>
			<td>Plans to remove from Workers RPC</td>
		</tr>
    <tr>
    	<td>Remote access to ctx and env</td>
    	<td>Yes</td>
    	<td>No</td>
    	<td>Cap'n Web requires a method for each access</td>
    </tr>
		<tr>
			<td>Promise pipelining</td>
			<td>Yes (validate chaining)</td>
			<td>Yes</td>
			<td>Confirm round-trip behavior under WS</td>
		</tr>
		<tr>
			<td>Client-side <code>.map()</code> record/replay</td>
			<td>No (consider guidance)</td>
			<td>Yes</td>
			<td>Recommend server-side composition where possible</td>
		</tr>
		<tr>
			<td>MessagePort transport</td>
			<td>Planned guidance</td>
			<td>Yes</td>
			<td>Good for iframes/workers; Lumenize is WS-first</td>
		</tr>
		<tr>
			<td>Workers RPC interop</td>
			<td>N/A</td>
			<td>Yes</td>
			<td>Cap'n Web integrates with Workers RPC</td>
		</tr>
		<tr>
			<td>Stub lifecycle & disposal (<code>[Symbol.dispose]</code>, <code>using</code>)</td>
			<td>Minimal lifecycle</td>
			<td>Yes</td>
			<td>Lumenize avoids heavy remote object graphs</td>
		</tr>
		<tr>
			<td><code>onRpcBroken</code> / connection break handling</td>
			<td>Doc/guardrails pending</td>
			<td>Yes</td>
			<td>Add guidance for resilience patterns</td>
		</tr>
		<tr>
			<td><code>dup()</code>, new stub/session semantics</td>
			<td>N/A</td>
			<td>Yes</td>
			<td>Cap'n Web offers explicit ownership semantics</td>
		</tr>
		<tr>
			<td>HTTP batch sessions</td>
			<td>N/A</td>
			<td>Yes</td>
			<td>Cap'n Web supports batched HTTP sessions</td>
		</tr>
		<tr>
			<td>WebSocket sessions</td>
			<td>Yes</td>
			<td>Yes</td>
			<td>Primary transport for Lumenize</td>
		</tr>
		<tr>
			<td><code>routeDORequest</code> helper</td>
			<td>Yes (no measured overhead)</td>
			<td>N/A</td>
			<td>Convention-based routing, binding lookup, type safety</td>
		</tr>
		<tr>
			<td>Runtime validation</td>
			<td>Planned (TypeBox Value)</td>
			<td>Recommended (varies)</td>
			<td>Add examples without forcing TypeBox dep</td>
		</tr>
		<tr>
			<td>Auth pattern (in-band → authed API)</td>
			<td>Yes (example pending)</td>
			<td>Yes</td>
			<td>Align with capability-based guidance</td>
		</tr>
		<tr>
			<td>Rate limiting guidance</td>
			<td>Planned</td>
			<td>Recommended</td>
			<td>Protect expensive RPC methods</td>
		</tr>
	</tbody>
  
</table>

## Differences

- For Cap'n Web, "[you must write a class that extends RpcTarget.](<https://github.com/cloudflare/capnweb/blob/main/README.md#rpctarget>)" Lumenize RPC works on plain Durable Objects which makes adding it easier. In fact adding it is the normal way [TODO: show example of wrapping with `lumenizeRpcDO`]

## What’s next (validation before publish)

We’re validating a few details before flipping this post to public:

- Promise pipelining behavior in Lumenize RPC: does chained usage add round trips?
- Capability-based security patterns and how they map to Lumenize
- MessagePort mode differences and applicability
- Connection break handling and stub lifecycle trade-offs
- Authentication pattern parity (in-band auth returning authenticated API)
- Runtime validation approach (TypeBox Value vs Zod)

Planned artifacts for this post:
- Network latency chart (./images/network-latency-chart.png)
- Throughput chart (./images/throughput.png)
- Protocol features diagram (./images/protocol-features.png)

If there’s a specific scenario you want us to measure, open an issue — we’ll add it to the test suite and update the post.

---

Note: This post is based on the internal scratchpad used during the measurements and has been consolidated for clarity. The full test harness lives in the Lumenize repo and will be linked here on publish.
